<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Animating the Imagination: Exploring AI Video Generation and Creative Boundaries</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <!-- Header -->
    <header>
        <div class="banner-container">
            <img src="../banner.jpg" alt="Portfolio Banner" class="banner">
            <div class="banner-text">Animating the Imagination: Exploring AI Video Generation and Creative Boundaries</div>
        </div>
        <nav>
            <ul class="menu">
                <li><a href="../index.html">Home</a></li>
                <li><a href="#description">Description</a></li>
                <li><a href="#tools">Tools</a></li>
            </ul>
        </nav>
    </header>

    <!-- Main Content -->
    <main>
        <section id="description">
            <h2>Project Description</h2>
            <img src="project1.jpg" alt="Project 1 Image" class="project-detail-image">
            <p>This project explores the potential and limitations of AI-driven video generation through tools like Runway and Luma. Inspired by a visit to Quebec City and the iconic statue of Bonhomme Carnaval at Montmorency Falls, the experiment sought to animate Bonhomme stepping off his pedestal and moving towards the camera. While the results were far from perfect, the process revealed fascinating insights into how these tools interpret prompts and extend visual elements.</p>

                <p>In both Runway and Luma-generated clips, the waterfall in the background of the original image continued to move seamlessly, even though ""waterfall"" was never explicitly mentioned. However, these tools struggled with the more complex task of animating Bonhomme, producing unexpected artifacts like additional waterfalls or tower-like structures morphing into trees. This mismatch between human expectations and AI interpretations highlights what Melanie Mitchell describes as ""conceptual slippage,"" where AI prioritizes visual elements differently than humans might intuitively. It also underscores the embodiment hypothesis—the idea that true human-like intelligence in machines requires embodied interaction with the world.</p>
                
                    <p>Building on this exercise, I reflected on how AI-driven art can thrive in the space between human imagination and algorithmic execution. The project was informed by two AI-mediated videos: Jon Finger's Summoner's Rune and Alan Warburton's The Wizard of AI. Finger’s improvisational storytelling demonstrated how AI tools can support collaborative, exploratory pre-production work, while Warburton’s polished video essay used generative AI to push visual and conceptual boundaries. Both works exemplify Zylinska’s notion of “dreaming with but also against the AI algorithm,” offering not just a response to current technologies but a vision for how they might inspire new forms of art-making.</p>
                
                        <p>By experimenting with these tools and reflecting on their creative implications, this project highlights the emergent surprises and constraints of AI-driven video production. Rather than replacing traditional filmmaking, such tools open opportunities for imaginative, hybrid forms of storytelling that can redefine the relationship between creators and their technologies."</p>
        </section>

        <section id="tools">
            <h2>Tools & Details</h2>
            <p><strong>Date:</strong> October 2024</p>
            <p><strong>Tools:</strong></p>
            <p>Runway (AI video generation)</p>
<p>Luma (AI video generation)</p>
<p>Original photograph of Bonhomme Carnaval at Montmorency Falls</p>
        </section>
    </main>

    <!-- Footer -->
    <footer>
        <p><a href="../index.html" class="portfolio-link">Back to Portfolio</a></p>
    </footer>
</body>
</html>